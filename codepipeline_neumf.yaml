AWSTemplateFormatVersion: '2010-09-09'
Description: CI/CD Pipeline for NeuMF Music Recommendation System - Includes Model Training and Deployment

# ===============================
# Parameters
# ===============================
Parameters:
  ProjectName:
    Type: String
    Default: music-recommendation-v2
    Description: Project name prefix

  GitHubOwner:
    Type: String
    Default: MaiSoeng
    Description: GitHub repository owner

  GitHubRepo:
    Type: String
    Default: spotify-recommendation
    Description: GitHub repository name

  GitHubBranch:
    Type: String
    Default: main
    Description: Branch to monitor

  CodeStarConnectionArn:
    Type: String
    Description: ARN of the CodeStar Connection to GitHub

  SageMakerTrainingInstanceType:
    Type: String
    Default: ml.g4dn.xlarge
    Description: SageMaker training instance type

  SageMakerInferenceInstanceType:
    Type: String
    Default: ml.t2.medium
    Description: SageMaker inference instance type

  ModelTrainingSchedule:
    Type: String
    Default: rate(7 days)
    Description: Schedule for automatic model retraining

  NotificationEmail:
    Type: String
    Description: Email for pipeline notifications

# ===============================
# Resources
# ===============================
Resources:

  # ===============================
  # S3 Buckets
  # ===============================
  
  PipelineArtifactBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${ProjectName}-pipeline-artifacts-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldArtifacts
            Status: Enabled
            ExpirationInDays: 30
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # ===============================
  # SNS Topic for Notifications
  # ===============================
  
  PipelineNotificationTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub '${ProjectName}-pipeline-notifications'

  PipelineNotificationSubscription:
    Type: AWS::SNS::Subscription
    Properties:
      TopicArn: !Ref PipelineNotificationTopic
      Protocol: email
      Endpoint: !Ref NotificationEmail

  # ===============================
  # CodeBuild Projects
  # ===============================
  
  # Build project for testing and packaging
  TestAndPackageBuild:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Sub '${ProjectName}-test-package'
      Description: Test code and package for deployment
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Artifacts:
        Type: CODEPIPELINE
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_MEDIUM
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        EnvironmentVariables:
          - Name: S3_BUCKET
            Value: !Ref PipelineArtifactBucket
          - Name: PROJECT_NAME
            Value: !Ref ProjectName
      Source:
        Type: CODEPIPELINE
        BuildSpec: |
          version: 0.2
          phases:
            install:
              runtime-versions:
                python: 3.11
              commands:
                - echo "Installing dependencies..."
                - pip install torch numpy pandas scikit-learn boto3 pytest
            pre_build:
              commands:
                - echo "Running tests..."
                - python -c "import torch; print(torch.__version__)"
                - echo "Validating CloudFormation templates..."
                - for f in stacks/*.yaml main.yaml; do aws cloudformation validate-template --template-body file://$f && echo "$f OK"; done
            build:
              commands:
                - echo "Uploading nested stack templates to S3..."
                - aws s3 cp stacks/ s3://${S3_BUCKET}/templates/stacks/ --recursive
                - aws s3 cp main.yaml s3://${S3_BUCKET}/templates/main.yaml
                - echo "Packaging training scripts..."
                - mkdir -p package && cp neumf_training.py neumf_inference.py package/
                - cd package && tar -czvf ../training-scripts.tar.gz . && cd ..
                - aws s3 cp training-scripts.tar.gz s3://${S3_BUCKET}/training-scripts/
                - aws s3 cp glue_etl_neumf.py s3://${S3_BUCKET}/scripts/
                - aws s3 cp glue_download_dataset.py s3://${S3_BUCKET}/scripts/
            post_build:
              commands:
                - echo "Build completed successfully"
          artifacts:
            files:
              - '**/*'
          cache:
            paths:
              - '/root/.cache/pip/**/*'
      TimeoutInMinutes: 30

  # Build project for triggering Glue download job
  DataDownloadBuild:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Sub '${ProjectName}-data-download'
      Description: Trigger Glue Python Shell Job to download dataset
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Artifacts:
        Type: CODEPIPELINE
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        EnvironmentVariables:
          - Name: PROJECT_NAME
            Value: !Ref ProjectName
          - Name: GLUE_JOB_NAME
            Value: !Sub '${ProjectName}-download-dataset'
      Source:
        Type: CODEPIPELINE
        BuildSpec: |
          version: 0.2
          phases:
            install:
              runtime-versions:
                python: 3.11
              commands:
                - pip install boto3
            build:
              commands:
                - echo "Triggering Glue Download Job..."
                - |
                  python << 'EOF'
                  import boto3
                  import os
                  import time
                  
                  glue = boto3.client('glue')
                  job_name = os.environ['GLUE_JOB_NAME']
                  
                  print(f"Starting Glue job: {job_name}")
                  
                  # Start the Glue job
                  response = glue.start_job_run(JobName=job_name)
                  run_id = response['JobRunId']
                  print(f"Job run started: {run_id}")
                  
                  # Wait for job to complete
                  print("Waiting for Glue job to complete...")
                  while True:
                      status = glue.get_job_run(JobName=job_name, RunId=run_id)
                      state = status['JobRun']['JobRunState']
                      print(f"Status: {state}")
                      
                      if state in ['SUCCEEDED']:
                          print("Glue job completed successfully!")
                          break
                      elif state in ['FAILED', 'STOPPED', 'TIMEOUT', 'ERROR']:
                          error_msg = status['JobRun'].get('ErrorMessage', 'Unknown error')
                          raise Exception(f"Glue job failed: {error_msg}")
                      
                      time.sleep(30)
                  
                  print("Dataset download complete!")
                  EOF
            post_build:
              commands:
                - echo "Data download stage completed"
                - echo "Done" > output.txt
          artifacts:
            files:
              - output.txt
      TimeoutInMinutes: 180  # 3 hours max (Glue job can run for 2 hours)

  # Build project for triggering Glue ETL job
  DataETLBuild:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Sub '${ProjectName}-data-etl'
      Description: Trigger Glue ETL Job to process dataset
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Artifacts:
        Type: CODEPIPELINE
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        EnvironmentVariables:
          - Name: PROJECT_NAME
            Value: !Ref ProjectName
          - Name: GLUE_JOB_NAME
            Value: !Sub '${ProjectName}-neumf-etl'
      Source:
        Type: CODEPIPELINE
        BuildSpec: |
          version: 0.2
          phases:
            install:
              runtime-versions:
                python: 3.11
              commands:
                - pip install boto3
            build:
              commands:
                - echo "Triggering Glue ETL Job..."
                - |
                  python << 'EOF'
                  import boto3
                  import os
                  import time
                  
                  glue = boto3.client('glue')
                  job_name = os.environ['GLUE_JOB_NAME']
                  
                  print(f"Starting Glue job: {job_name}")
                  
                  # Start the Glue job
                  response = glue.start_job_run(JobName=job_name)
                  run_id = response['JobRunId']
                  print(f"Job run started: {run_id}")
                  
                  # Wait for job to complete
                  print("Waiting for Glue job to complete...")
                  while True:
                      status = glue.get_job_run(JobName=job_name, RunId=run_id)
                      state = status['JobRun']['JobRunState']
                      print(f"Status: {state}")
                      
                      if state in ['SUCCEEDED']:
                          print("Glue job completed successfully!")
                          break
                      elif state in ['FAILED', 'STOPPED', 'TIMEOUT', 'ERROR']:
                          error_msg = status['JobRun'].get('ErrorMessage', 'Unknown error')
                          raise Exception(f"Glue job failed: {error_msg}")
                      
                      time.sleep(30)
                  
                  print("ETL job complete!")
                  EOF
            post_build:
              commands:
                - echo "Data ETL stage completed"
                - echo "Done" > output.txt
          artifacts:
            files:
              - output.txt
      TimeoutInMinutes: 180

  # Build project for model training
  ModelTrainingBuild:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Sub '${ProjectName}-model-training'
      Description: Train NeuMF model using SageMaker
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Artifacts:
        Type: CODEPIPELINE
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        EnvironmentVariables:
          - Name: PROJECT_NAME
            Value: !Ref ProjectName
          - Name: SAGEMAKER_ROLE
            Value: !Sub 'arn:aws:iam::${AWS::AccountId}:role/${ProjectName}-sagemaker-role'
          - Name: TRAINING_INSTANCE_TYPE
            Value: !Ref SageMakerTrainingInstanceType
          - Name: REGION
            Value: !Ref AWS::Region
          - Name: PIPELINE_BUCKET
            Value: !Ref PipelineArtifactBucket
      Source:
        Type: CODEPIPELINE
        BuildSpec: |
          version: 0.2
          phases:
            install:
              runtime-versions:
                python: 3.11
              commands:
                - pip install boto3 sagemaker
            build:
              commands:
                - echo "Starting SageMaker training job..."
                - echo "FAILED_JOB" > training_job_name.txt
                - |
                  python << 'EOF'
                  import boto3
                  import os
                  import time
                  import sys
                  import traceback
                  from datetime import datetime

                  try:
                      sagemaker = boto3.client('sagemaker')
                      
                      print("Reading environment variables...")
                      project_name = os.environ['PROJECT_NAME']
                      role_arn = os.environ['SAGEMAKER_ROLE']
                      instance_type = os.environ['TRAINING_INSTANCE_TYPE']
                      region = os.environ['REGION']
                      pipeline_bucket = os.environ['PIPELINE_BUCKET']
                      account_id = boto3.client('sts').get_caller_identity()['Account']
                      
                      job_name = f"{project_name}-training-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
                      
                      # Get bucket names from CloudFormation exports
                      cf = boto3.client('cloudformation')
                      
                      try:
                          processed_bucket = cf.describe_stacks(
                              StackName=f'{project_name}'
                          )['Stacks'][0]['Outputs']
                          processed_bucket = next(
                              o['OutputValue'] for o in processed_bucket 
                              if o['OutputKey'] == 'ProcessedDataBucketName'
                          )
                      except:
                          processed_bucket = f"{project_name}-processed-data-{account_id}"
                      
                      try:
                          model_bucket = cf.describe_stacks(
                              StackName=f'{project_name}'
                          )['Stacks'][0]['Outputs']
                          model_bucket = next(
                              o['OutputValue'] for o in model_bucket 
                              if o['OutputKey'] == 'ModelArtifactsBucketName'
                          )
                      except:
                          model_bucket = f"{project_name}-model-artifacts-{account_id}"
                      
                      print(f"Starting training job: {job_name}")
                      print(f"Training data: s3://{processed_bucket}/neumf-training-data/train/")
                      print(f"Model output: s3://{model_bucket}/models/")
                      print(f"Training script: s3://{pipeline_bucket}/training-scripts/training-scripts.tar.gz")
                      
                      # Create training job
                      response = sagemaker.create_training_job(
                          TrainingJobName=job_name,
                          RoleArn=role_arn,
                          AlgorithmSpecification={
                              'TrainingImage': f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.0.1-gpu-py310',
                              'TrainingInputMode': 'File',
                              'MetricDefinitions': [
                                  {'Name': 'train:loss', 'Regex': 'Loss: ([0-9\\.]+)'},
                                  {'Name': 'train:hr10', 'Regex': 'HR@10: ([0-9\\.]+)'},
                                  {'Name': 'train:ndcg10', 'Regex': 'NDCG@10: ([0-9\\.]+)'}
                              ]
                          },
                          InputDataConfig=[
                              {
                                  'ChannelName': 'train',
                                  'DataSource': {
                                      'S3DataSource': {
                                          'S3DataType': 'S3Prefix',
                                          'S3Uri': f's3://{processed_bucket}/neumf-training-data/train/',
                                          'S3DataDistributionType': 'FullyReplicated'
                                      }
                                  }
                              }
                          ],
                          OutputDataConfig={
                              'S3OutputPath': f's3://{model_bucket}/models/'
                          },
                          ResourceConfig={
                              'InstanceType': instance_type,
                              'InstanceCount': 1,
                              'VolumeSizeInGB': 50
                          },
                          StoppingCondition={
                              'MaxRuntimeInSeconds': 7200  # 2 hours max
                          },
                          HyperParameters={
                              'epochs': '5',        # 5 epochs on GPU is fast
                              'batch-size': '2048', # Optimized for T4 GPU
                              'num-negatives': '4', # High quality
                              'learning-rate': '0.001',
                              'gmf-embedding-dim': '32',
                              'mlp-embedding-dim': '32',
                              'mlp-layers': '128,64,32',
                              'sagemaker_program': 'neumf_training.py',
                              'sagemaker_submit_directory': f"s3://{pipeline_bucket}/training-scripts/training-scripts.tar.gz"
                          },
                          EnableManagedSpotTraining=False,  # Use on-demand instances for reliability
                          Tags=[
                              {'Key': 'Project', 'Value': project_name}
                          ]
                      )
                      
                      print(f"Training job started: {response['TrainingJobArn']}")
                      
                      # Save job name for next stage
                      with open('training_job_name.txt', 'w') as f:
                          f.write(job_name)
                      
                      # Wait for training to complete (with timeout)
                      print("Waiting for training to complete...")
                      waiter = sagemaker.get_waiter('training_job_completed_or_stopped')
                      waiter.wait(
                          TrainingJobName=job_name,
                          WaiterConfig={'Delay': 60, 'MaxAttempts': 120}  # Up to 2 hours
                      )
                      
                      # Check final status
                      status = sagemaker.describe_training_job(TrainingJobName=job_name)
                      print(f"Training completed with status: {status['TrainingJobStatus']}")
                      
                      if status['TrainingJobStatus'] != 'Completed':
                          raise Exception(f"Training failed: {status.get('FailureReason', 'Unknown')}")
                      
                      print(f"Model artifacts: {status['ModelArtifacts']['S3ModelArtifacts']}")

                  except Exception as e:
                      print("CRITICAL ERROR IN SCRIPT:")
                      print(str(e))
                      traceback.print_exc()
                      sys.exit(1)
                  EOF
                  

          artifacts:
            files:
              - training_job_name.txt
      TimeoutInMinutes: 180

  # Build project for model deployment
  ModelDeploymentBuild:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Sub '${ProjectName}-model-deployment'
      Description: Deploy trained model to SageMaker endpoint
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Artifacts:
        Type: CODEPIPELINE
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        EnvironmentVariables:
          - Name: PROJECT_NAME
            Value: !Ref ProjectName
          - Name: SAGEMAKER_ROLE
            Value: !Sub 'arn:aws:iam::${AWS::AccountId}:role/${ProjectName}-sagemaker-role'
          - Name: INFERENCE_INSTANCE_TYPE
            Value: !Ref SageMakerInferenceInstanceType
          - Name: REGION
            Value: !Ref AWS::Region
      Source:
        Type: CODEPIPELINE
        BuildSpec: |
          version: 0.2
          phases:
            install:
              runtime-versions:
                python: 3.11
              commands:
                - pip install boto3
            build:
              commands:
                - echo "Deploying model to SageMaker endpoint..."
                - |
                  python << 'EOF'
                  import boto3
                  import os
                  from datetime import datetime
                  
                  sagemaker = boto3.client('sagemaker')
                  
                  project_name = os.environ['PROJECT_NAME']
                  role_arn = os.environ['SAGEMAKER_ROLE']
                  instance_type = os.environ['INFERENCE_INSTANCE_TYPE']
                  region = os.environ['REGION']

                  # DEBUG: Print environment and directories
                  print("DEBUG: Listing codebuild-related env vars:")
                  for k, v in os.environ.items():
                      if k.startswith('CODEBUILD'):
                          print(f"{k}={v}")
                  
                  print(f"DEBUG: Current directory {os.getcwd()} contents:")
                  print(os.listdir('.'))

                  # Read training job name from TrainingArtifact (Secondary Source)
                  training_artifact_dir = os.environ.get('CODEBUILD_SRC_DIR_TrainingArtifact')
                  
                  if training_artifact_dir:
                      print(f"DEBUG: Training artifact dir set to: {training_artifact_dir}")
                      print(f"DEBUG: Contents: {os.listdir(training_artifact_dir)}")
                      job_name_path = os.path.join(training_artifact_dir, 'training_job_name.txt')
                  else:
                      print("DEBUG: CODEBUILD_SRC_DIR_TrainingArtifact not set, looking in current dir")
                      training_artifact_dir = '.'
                      job_name_path = 'training_job_name.txt'
                  
                  # Fallback for local testing
                  if not os.path.exists(job_name_path):
                      print(f"WARNING: {job_name_path} does not exist. Checking alternative locations.")
                      if os.path.exists('training_job_name.txt'):
                           job_name_path = 'training_job_name.txt'

                  print(f"Reading job name from: {job_name_path}")
                  with open(job_name_path, 'r') as f:
                      training_job_name = f.read().strip()
                  
                  # Get model artifacts location
                  training_job = sagemaker.describe_training_job(TrainingJobName=training_job_name)
                  original_model_url = training_job['ModelArtifacts']['S3ModelArtifacts']
                  
                  # ==========================================================
                  # Repack Model with Latest Inference Script
                  # ==========================================================
                  print(f"Repacking model from {original_model_url}")
                  
                  # Download original model
                  bucket_name = original_model_url.split('/')[2]
                  key = '/'.join(original_model_url.split('/')[3:])
                  s3 = boto3.client('s3')
                  s3.download_file(bucket_name, key, 'model.tar.gz')
                  
                  # Add new inference script to code/ folder in tar
                  import tarfile
                  
                  # Create code directory structure
                  os.makedirs('code', exist_ok=True)
                  
                  # Copy neumf_inference.py from Current Directory (Primary Source)
                  source_file = 'neumf_inference.py'
                  
                  import shutil
                  print(f"Copying inference script from {source_file}")
                  if os.path.exists(source_file):
                      shutil.copy(source_file, 'code/inference.py')
                      # Also copy as neumf_inference.py just in case
                      shutil.copy(source_file, 'code/neumf_inference.py')
                  else:
                      print(f"ERROR: {source_file} not found in current directory!")
                      raise FileNotFoundError("neumf_inference.py not found")
                      
                  # Copy requirements if exists
                  if os.path.exists('requirements.txt'):
                      shutil.copy('requirements.txt', 'code/requirements.txt')

                  # Update tarball
                  with tarfile.open('model.tar.gz', 'a:gz') as tar:
                      tar.add('code', arcname='code')
                  
                  # Upload repacked model
                  repacked_key = f"repacked-models/{training_job_name}/model.tar.gz"
                  s3.upload_file('model.tar.gz', bucket_name, repacked_key)
                  model_data_url = f"s3://{bucket_name}/{repacked_key}"
                  print(f"Repacked model uploaded to {model_data_url}")
                  
                  timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
                  model_name = f"{project_name}-neumf-{timestamp}"
                  endpoint_config_name = f"{project_name}-neumf-config-{timestamp}"
                  endpoint_name = f"{project_name}-neumf-endpoint"
                  
                  print(f"Creating model: {model_name}")
                  
                  # Create model
                  sagemaker.create_model(
                      ModelName=model_name,
                      PrimaryContainer={
                          'Image': f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference:2.0.1-cpu-py310',
                          'ModelDataUrl': model_data_url,
                          'Environment': {
                              'SAGEMAKER_PROGRAM': 'inference.py',
                              'SAGEMAKER_SUBMIT_DIRECTORY': 'code'
                          }
                      },
                      ExecutionRoleArn=role_arn,
                      Tags=[
                          {'Key': 'Project', 'Value': project_name}
                      ]
                  )
                  
                  print(f"Creating endpoint config: {endpoint_config_name}")
                  
                  # Create endpoint config
                  sagemaker.create_endpoint_config(
                      EndpointConfigName=endpoint_config_name,
                      ProductionVariants=[
                          {
                              'VariantName': 'AllTraffic',
                              'ModelName': model_name,
                              'InstanceType': instance_type,
                              'InitialInstanceCount': 1,
                              'InitialVariantWeight': 1.0
                          }
                      ],
                      Tags=[
                          {'Key': 'Project', 'Value': project_name}
                      ]
                  )
                  
                  # Create or update endpoint
                  try:
                      sagemaker.describe_endpoint(EndpointName=endpoint_name)
                      print(f"Updating existing endpoint: {endpoint_name}")
                      sagemaker.update_endpoint(
                          EndpointName=endpoint_name,
                          EndpointConfigName=endpoint_config_name
                      )
                  except sagemaker.exceptions.ClientError:
                      print(f"Creating new endpoint: {endpoint_name}")
                      sagemaker.create_endpoint(
                          EndpointName=endpoint_name,
                          EndpointConfigName=endpoint_config_name,
                          Tags=[
                              {'Key': 'Project', 'Value': project_name}
                          ]
                      )
                  
                  # Wait for endpoint to be ready
                  print("Waiting for endpoint to be in service...")
                  waiter = sagemaker.get_waiter('endpoint_in_service')
                  waiter.wait(
                      EndpointName=endpoint_name,
                      WaiterConfig={'Delay': 30, 'MaxAttempts': 60}
                  )
                  
                  print(f"Endpoint deployed successfully: {endpoint_name}")
                  EOF
            post_build:
              commands:
                - echo "Model deployment completed"
      TimeoutInMinutes: 60

  # ===============================
  # CodePipeline
  # ===============================
  
  MainPipeline:
    Type: AWS::CodePipeline::Pipeline
    Properties:
      Name: !Sub '${ProjectName}-main-pipeline'
      RoleArn: !GetAtt PipelineRole.Arn
      ArtifactStore:
        Type: S3
        Location: !Ref PipelineArtifactBucket
      Stages:
        # Stage 1: Source
        - Name: Source
          Actions:
            - Name: GitHubSource
              ActionTypeId:
                Category: Source
                Owner: AWS
                Provider: CodeStarSourceConnection
                Version: '1'
              Configuration:
                ConnectionArn: !Ref CodeStarConnectionArn
                FullRepositoryId: !Sub '${GitHubOwner}/${GitHubRepo}'
                BranchName: !Ref GitHubBranch
                OutputArtifactFormat: CODE_ZIP
              OutputArtifacts:
                - Name: SourceArtifact



        # Stage 2: Test and Package
        - Name: TestAndPackage
          Actions:
            - Name: TestAndPackage
              ActionTypeId:
                Category: Build
                Owner: AWS
                Provider: CodeBuild
                Version: '1'
              InputArtifacts:
                - Name: SourceArtifact
              OutputArtifacts:
                - Name: PackageArtifact
              Configuration:
                ProjectName: !Ref TestAndPackageBuild

        # Stage 3: Deploy Infrastructure (Nested Stacks)
        - Name: DeployInfrastructure
          Actions:
            - Name: CreateChangeSet
              ActionTypeId:
                Category: Deploy
                Owner: AWS
                Provider: CloudFormation
                Version: '1'
              InputArtifacts:
                - Name: PackageArtifact
                - Name: SourceArtifact
              Configuration:
                ActionMode: CHANGE_SET_REPLACE
                StackName: !Sub '${ProjectName}-main'
                ChangeSetName: !Sub '${ProjectName}-changeset'
                TemplatePath: PackageArtifact::main.yaml
                TemplateConfiguration: SourceArtifact::parameters.json
                ParameterOverrides: !Sub |
                  {
                    "ProjectName": "${ProjectName}",
                    "StorageStackUrl": "https://s3.${AWS::Region}.amazonaws.com/${PipelineArtifactBucket}/templates/stacks/storage.yaml",
                    "NetworkingStackUrl": "https://s3.${AWS::Region}.amazonaws.com/${PipelineArtifactBucket}/templates/stacks/networking.yaml",
                    "IamStackUrl": "https://s3.${AWS::Region}.amazonaws.com/${PipelineArtifactBucket}/templates/stacks/iam.yaml",
                    "GlueStackUrl": "https://s3.${AWS::Region}.amazonaws.com/${PipelineArtifactBucket}/templates/stacks/glue.yaml",
                    "LambdaStackUrl": "https://s3.${AWS::Region}.amazonaws.com/${PipelineArtifactBucket}/templates/stacks/lambda.yaml",
                    "PipelineBucket": "${PipelineArtifactBucket}"
                  }
                Capabilities: CAPABILITY_NAMED_IAM,CAPABILITY_AUTO_EXPAND
                RoleArn: !GetAtt CloudFormationRole.Arn
              RunOrder: 1
            - Name: ExecuteChangeSet
              ActionTypeId:
                Category: Deploy
                Owner: AWS
                Provider: CloudFormation
                Version: '1'
              Configuration:
                ActionMode: CHANGE_SET_EXECUTE
                StackName: !Sub '${ProjectName}-main'
                ChangeSetName: !Sub '${ProjectName}-changeset'
              RunOrder: 2

        # Stage 6: Generate Training Data
        - Name: PrepareData
          Actions:
            - Name: ApproveDataGeneration
              ActionTypeId:
                Category: Approval
                Owner: AWS
                Provider: Manual
                Version: '1'
              Configuration:
                NotificationArn: !Ref PipelineNotificationTopic
                CustomData: "Approve to generate training data using Lambda function"
            - Name: DownloadData
              ActionTypeId:
                Category: Build
                Owner: AWS
                Provider: CodeBuild
                Version: '1'
              InputArtifacts:
                - Name: SourceArtifact
              Configuration:
                ProjectName: !Ref DataDownloadBuild
              RunOrder: 2
            - Name: RunETL
              ActionTypeId:
                Category: Build
                Owner: AWS
                Provider: CodeBuild
                Version: '1'
              InputArtifacts:
                - Name: SourceArtifact
              Configuration:
                ProjectName: !Ref DataETLBuild
              RunOrder: 3

        # Stage 7: Train Model
        - Name: TrainModel
          Actions:
            - Name: ApproveTraining
              ActionTypeId:
                Category: Approval
                Owner: AWS
                Provider: Manual
                Version: '1'
              Configuration:
                NotificationArn: !Ref PipelineNotificationTopic
                CustomData: "Approve to start SageMaker training job (estimated cost: ~$1-2)"
            - Name: RunTraining
              ActionTypeId:
                Category: Build
                Owner: AWS
                Provider: CodeBuild
                Version: '1'
              InputArtifacts:
                - Name: PackageArtifact
              OutputArtifacts:
                - Name: TrainingArtifact
              Configuration:
                ProjectName: !Ref ModelTrainingBuild
              RunOrder: 2

        # Stage 8: Deploy Model
        - Name: DeployModel
          Actions:
            - Name: ApproveDeployment
              ActionTypeId:
                Category: Approval
                Owner: AWS
                Provider: Manual
                Version: '1'
              Configuration:
                NotificationArn: !Ref PipelineNotificationTopic
                CustomData: "Approve to deploy model to SageMaker endpoint (ongoing cost: ~$0.05/hour)"
            - Name: DeployToEndpoint
              ActionTypeId:
                Category: Build
                Owner: AWS
                Provider: CodeBuild
                Version: '1'
              InputArtifacts:
                - Name: SourceArtifact
                - Name: TrainingArtifact
              Configuration:
                ProjectName: !Ref ModelDeploymentBuild
                PrimarySource: SourceArtifact
              RunOrder: 2

  # ===============================
  # Scheduled Model Retraining
  # ===============================
  
  RetrainingScheduleRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub '${ProjectName}-retraining-schedule'
      Description: Scheduled model retraining trigger
      ScheduleExpression: !Ref ModelTrainingSchedule
      State: ENABLED
      Targets:
        - Id: TriggerRetrainingPipeline
          Arn: !Sub 'arn:aws:codepipeline:${AWS::Region}:${AWS::AccountId}:${MainPipeline}'
          RoleArn: !GetAtt EventBridgeRole.Arn

  EventBridgeRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-eventbridge-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: events.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: TriggerPipeline
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: codepipeline:StartPipelineExecution
                Resource: !Sub 'arn:aws:codepipeline:${AWS::Region}:${AWS::AccountId}:${MainPipeline}'

  # ===============================
  # IAM Roles
  # ===============================
  
  CodeBuildRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-codebuild-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: codebuild.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: CodeBuildPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:GetBucketLocation
                Resource:
                  - !Sub 'arn:aws:s3:::${PipelineArtifactBucket}/*'
                  - !Sub 'arn:aws:s3:::${ProjectName}-*'
              - Effect: Allow
                Action:
                  - cloudformation:ValidateTemplate
                  - cloudformation:DescribeStacks
                Resource: '*'
              - Effect: Allow
                Action:
                  - glue:StartJobRun
                  - glue:GetJobRun
                Resource: !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:job/${ProjectName}-download-dataset'
              - Effect: Allow
                Action:
                  - glue:StartJobRun
                  - glue:GetJobRun
                Resource: !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:job/${ProjectName}-neumf-etl'
              - Effect: Allow
                Action:
                  - sagemaker:CreateTrainingJob
                  - sagemaker:DescribeTrainingJob
                  - sagemaker:CreateModel
                  - sagemaker:CreateEndpointConfig
                  - sagemaker:CreateEndpoint
                  - sagemaker:UpdateEndpoint
                  - sagemaker:DescribeEndpoint
                  - sagemaker:DeleteEndpointConfig
                  - sagemaker:DeleteModel
                  - sagemaker:AddTags
                Resource: '*'
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource:
                  - !Sub 'arn:aws:iam::${AWS::AccountId}:role/${ProjectName}-sagemaker-role'


  PipelineRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-pipeline-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: codepipeline.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: PipelinePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - codestar-connections:UseConnection
                Resource: !Ref CodeStarConnectionArn
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:GetBucketVersioning
                Resource:
                  - !Sub 'arn:aws:s3:::${PipelineArtifactBucket}/*'
              - Effect: Allow
                Action:
                  - codebuild:BatchGetBuilds
                  - codebuild:StartBuild
                Resource:
                  - !GetAtt TestAndPackageBuild.Arn
                  - !GetAtt DataDownloadBuild.Arn
                  - !GetAtt DataETLBuild.Arn
                  - !GetAtt ModelTrainingBuild.Arn
                  - !GetAtt ModelDeploymentBuild.Arn
              - Effect: Allow
                Action:
                  - cloudformation:CreateStack
                  - cloudformation:UpdateStack
                  - cloudformation:DeleteStack
                  - cloudformation:DescribeStacks
                  - cloudformation:DescribeStackEvents
                  - cloudformation:CreateChangeSet
                  - cloudformation:ExecuteChangeSet
                  - cloudformation:DescribeChangeSet
                Resource: '*'
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource: !GetAtt CloudFormationRole.Arn
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !Sub 'arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:${ProjectName}-*'
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref PipelineNotificationTopic

  CloudFormationRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-cloudformation-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: cloudformation.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AdministratorAccess  # For demo; restrict in production
      Policies:
        - PolicyName: LakeFormationAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
            - Effect: Allow
              Action:
              - lakeformation:*
              Resource: '*'
            - Effect: Allow
              Action:
              - iam:PassRole
              Resource:
              - !Sub 'arn:aws:iam::${AWS::AccountId}:role/${ProjectName}-glue-role'
            - Effect: Allow
              Action:
              - lakeformation:GrantPermissions
              - lakeformation:RevokePermissions
              - lakeformation:ListPermissions
              - lakeformation:GetResourceLFTags
              - lakeformation:PutResourceLFTags
              - lakeformation:DeleteResourceLFTags
              - lakeformation:GetDataLakeSettings
              - lakeformation:PutDataLakeSettings
              - lakeformation:GetEffectivePermissionsForPath
              Resource: '*'
            - Effect: Allow
              Action:
              - lakeformation:GetDataAccess
              - lakeformation:GetTableObjects
              - lakeformation:GetWorkUnits
              - lakeformation:StartQueryPlanning
              - lakeformation:GetWorkUnitResults
              - lakeformation:GetQueryState
              - lakeformation:GetQueryStatistics
              - lakeformation:StopQueryPlanning
              - lakeformation:UpdateTableObjects
              - lakeformation:DeleteTableObjects
              Resource: '*'
            - Effect: Allow
              Action:
              - lakeformation:PutDataLakeSettings
              - lakeformation:GetDataLakeSettings
              - lakeformation:RegisterResource
              - lakeformation:DeregisterResource
              - lakeformation:GrantPermissions
              - lakeformation:RevokePermissions
              - lakeformation:ListPermissions
              - lakeformation:GetResource
              - lakeformation:ListResources
              Resource: '*'

# ===============================
# Outputs
# ===============================
Outputs:
  PipelineName:
    Description: CodePipeline name
    Value: !Ref MainPipeline

  PipelineUrl:
    Description: CodePipeline console URL
    Value: !Sub 'https://console.aws.amazon.com/codesuite/codepipeline/pipelines/${MainPipeline}/view?region=${AWS::Region}'

  ArtifactBucket:
    Description: Pipeline artifact bucket
    Value: !Ref PipelineArtifactBucket

  NotificationTopic:
    Description: SNS topic for pipeline notifications
    Value: !Ref PipelineNotificationTopic
